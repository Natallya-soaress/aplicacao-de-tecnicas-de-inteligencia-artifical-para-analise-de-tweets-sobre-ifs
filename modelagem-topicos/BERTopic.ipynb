{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b4d818-c7c2-483a-853e-8a0c16a015b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install bertopic\n",
    "from bertopic import BERTopic\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4eb326fb-8021-488e-b792-2de07987d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e96adfa-be96-4763-acb3-1fd389a8561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ler_arquivo(arquivo):\n",
    "    arquivo = open(arquivo, \"r\")\n",
    "    conteudo = arquivo.read().splitlines() \n",
    "    return conteudo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d2b301-71fd-4487-b80f-3edb322001d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = ler_arquivo(\"./centro-oeste.txt\")\n",
    "# docs = ler_arquivo(\"./sul.txt\")\n",
    "# docs = ler_arquivo(\"./nordeste.txt\")\n",
    "# docs = ler_arquivo(\"./sudeste.txt\")\n",
    "# docs = ler_arquivo(\"./norte.txt\")\n",
    "docs = ler_arquivo(\"./dados-filtrados.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89702bc2-c39c-4cc3-9561-76c6e52d316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_range = range(2, 20)\n",
    "coherence_scores_per_topic = {i: [] for i in num_topics_range}\n",
    "\n",
    "num_executions = 10  \n",
    "\n",
    "for _ in range(num_executions):\n",
    "    coherence_scores = {i: [] for i in num_topics_range}\n",
    "    \n",
    "    for num_topics in num_topics_range:\n",
    "        topic_model = BERTopic(nr_topics=num_topics, language=\"multilingual\")\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "        cleaned_docs = topic_model._preprocess_text(docs)\n",
    "        vectorizer = topic_model.vectorizer_model\n",
    "        analyzer = vectorizer.build_analyzer()\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topics = topic_model.get_topics()\n",
    "        topics.pop(-1, None)\n",
    "        topic_words = [\n",
    "            [word for word, _ in topic_model.get_topic(topic) if word != \"\"] for topic in topics\n",
    "        ]\n",
    "\n",
    "        coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                        texts=tokens,\n",
    "                                        corpus=corpus,\n",
    "                                        dictionary=dictionary,\n",
    "                                        coherence='c_v')\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "        coherence_scores[num_topics].append(coherence_score)\n",
    "\n",
    "    for topic, scores in coherence_scores.items():\n",
    "        coherence_scores_per_topic[topic].extend(scores)\n",
    "\n",
    "average_coherence_per_topic = {topic: np.mean(scores) for topic, scores in coherence_scores_per_topic.items()}\n",
    "\n",
    "plt.plot(list(average_coherence_per_topic.keys()), list(average_coherence_per_topic.values()), marker='o')\n",
    "plt.xticks(np.arange(min(average_coherence_per_topic.keys()), max(average_coherence_per_topic.keys())+1, 1))  # Forcing integer x-axis ticks\n",
    "plt.xlabel(\"Número de Tópicos\")\n",
    "plt.ylabel(\"Média de Coerência\")\n",
    "plt.title(\"Média de coerência por tópico em 10 execuções\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013e435-878c-49b0-b537-18eb0f4eb0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descobre_topicos_if(texto, nr_topics):\n",
    "    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    topic_model = BERTopic(nr_topics=15, language=\"multilingual\", top_n_words=30, n_gram_range=(1, 1), ctfidf_model=ctfidf_model)\n",
    "    topics, _ = topic_model.fit_transform(docs)\n",
    "    cleaned_docs = topic_model._preprocess_text(docs)\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "    tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topics = topic_model.get_topics()\n",
    "    topics.pop(-1, None)\n",
    "    topic_words = [\n",
    "        [word for word, _ in topic_model.get_topic(topic) if word != \"\"] for topic in topics\n",
    "    ]\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=topic_words,\n",
    "                                        texts=tokens,\n",
    "                                        corpus=corpus,\n",
    "                                        dictionary=dictionary,\n",
    "                                        coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence_per_topic() \n",
    "\n",
    "    topic_tweets = {}\n",
    "    for tweet, topic_id in zip(texto, topics):\n",
    "        if topic_id not in topic_tweets:\n",
    "            topic_tweets[topic_id] = []\n",
    "        topic_tweets[topic_id].append(tweet)\n",
    "\n",
    "    return topics, topic_tweets, coherence_score\n",
    "\n",
    "\n",
    "def printa_topicos_if(topic_words):\n",
    "    num_linhas = 2\n",
    "    num_colunas = 2\n",
    "\n",
    "    fig, axs = plt.subplots(num_linhas, num_colunas, figsize=(12, 8))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for topic_id in range(len(topic_words)):\n",
    "        words = [word for word, _ in topic_words[topic_id] if word != \"\"]\n",
    "        wordcloud = WordCloud(width=400, height=200, background_color='white').generate(' '.join(words))\n",
    "\n",
    "        linha_atual = topic_id // num_colunas\n",
    "        coluna_atual = topic_id % num_colunas\n",
    "\n",
    "        axs[linha_atual, coluna_atual].imshow(wordcloud, interpolation='bilinear')\n",
    "        axs[linha_atual, coluna_atual].axis('off')\n",
    "        axs[linha_atual, coluna_atual].set_title(f\"Tópico {topic_id+1}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76399aa-92a8-42b1-b2f2-a4cf8e2b21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, topic_tweets, coherence_score = descobre_topicos_if(docs, 4)\n",
    "printa_topicos_if(topics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
